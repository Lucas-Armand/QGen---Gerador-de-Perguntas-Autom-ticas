# QGen â€” Gerador de Perguntas AutomÃ¡ticas

ğŸ¥ [Assista ao vÃ­deo explicativo no YouTube](https://youtu.be/vzNmwpo6GxI)

Plataforma simples (Streamlit + Docker) para **gerar perguntas de mÃºltipla escolha** a partir do conteÃºdo um texto de aula. 

A app deve rodar em dois modos:
1) **Local** usando modelos abertos (ex.: `llama3.2:3b-instruct`, `mistral:7b-instruct`);
2) **Nuvem** usando algum modelo gratuita (via Google Gemini (AI Studio) ou OpenRouter).

---

## Problema & Objetivo

Educadores e criadores de conteÃºdo precisam transformar rapidamente **textos de aula** em **itens de avaliaÃ§Ã£o**. O objetivo Ã© entregar um **MVP funcional** que:
- receba um **.txt**,
- gere **perguntas com alternativas** (gabarito + explicaÃ§Ã£o curta),
- soluÃ§Ã£o baseada em LLM,
- permita alternar entre **LLM local** e **LLM online** (chave opcional),

---

## Escopo do MVP (primeira entrega)

- **Tela 1 (Upload)**: usuÃ¡rio sobe um `.txt` com o conteÃºdo da aula.
- **Tela 2 (Resultados)**: exibe as perguntas geradas com:
  - enunciado,
  - 4 alternativas (1 correta + 3 distratores),
  - â€œracionalâ€/explicaÃ§Ã£o curta,
  - botÃ£o para **exporta pergunta**.

---

## VisÃ£o TÃ©cnica (curta)

- **Frontend**: Streamlit  
- **LLM**:
  - **Local** (Ollama): `llama3.2:3b-instruct` ou `mistral:7b-instruct`
  - **Online** (opcional): Gemini (AI Studio) ou OpenRouter
- **Empacotamento**: Docker (um container para a app; Ollama pode ser externo ou um 2Âº container)

### Fluxo (MVP)

[Upload .txt] â†’ [PrÃ©-processamento leve] â†’ [Gerador (LLM local/online ou baseline)] â†’ [Render na UI] â†’ [Export]

## Roadmap (Baby Steps)

**ETAPA 1 â€”  InÃ­cio**
- [x] Estrutura do app (Streamlit + rotas entre telas)
- [x] Upload `.txt` e armazenamento temporÃ¡rio
- [x] LLM Mockado
- [x] Tela de resultados 
- [x] Ler papers sobre GeraÃ§Ã£o de Perguntas com LLM

**ETAPA 2 â€” LLM Local**
- [x] Client Ollama (HTTP) com modelo configurÃ¡vel
- [x] Prompt base 

**ETAPA 3 â€” LLM Online (opcional)**
- [ ] IntegraÃ§Ã£o Gemini (AI Studio)

**ETAPA 4 â€” Qualidade & DX**
- [ ] HeurÃ­sticas: filtrar sentenÃ§as curtas/repetidas; evitar termos triviais
- [ ] MÃ©tricas simples sobre a qualidade das perguntas e respostas
- [ ] Testes mÃ­nimos 
- [x] Dockerfile + docker-compose (app e, opcionalmente, serviÃ§o llm)

---

## Estrutura do projeto

```
./
â”œâ”€â”€ app
â”‚Â Â  â”œâ”€â”€ app.py
â”‚Â Â  â”œâ”€â”€ llm_mock.py
â”‚Â Â  â”œâ”€â”€ llm_ollama.py
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â””â”€â”€ sample
    â””â”€â”€ aula.txt

```

---

## Riscos & MitigaÃ§Ãµes

- **Limites de API Free**: manter fallback local e baseline.
- **Qualidade dos distratores**: comeÃ§ar simples (vocabulÃ¡rio do texto) e evoluir.
- **Tempo de inferÃªncia**: preferir prompts curtos, modelos leves; streaming opcional.
- **Variabilidade do LLM**: padronizar instruÃ§Ãµes e pÃ³s-processar para formato JSON estrito.

---

## DiÃ¡rio â€” 2025-08-26 (esboÃ§o & planejamento)

- **DecisÃµes do dia**:
  - Escopo do MVP com **2 telas** (Upload â†’ Resultados).
  - Suporte a **4 backends**: `baseline`, `ollama`, `gemini`, `openrouter`.
  - SaÃ­da Ãºnica em **JSON** para facilitar testes e integraÃ§Ã£o.
- **Tarefas mapeadas**:
  - Criar estrutura de pastas e arquivos base.
  - Implementar baseline **sem LLM** (TF-IDF + cloze + distratores do prÃ³prio texto).
  - Definir prompt inicial (portuguÃªs, objetivo, 1 correta + 3 distratores + racional).
  - Especificar `.env` e selector de backend no Streamlit.
  - EsboÃ§ar Dockerfile e docker-compose (app + opcional llm).
- **PendÃªncias**:
  - Escolha final dos modelos (comeÃ§ar com `mistral:7b-instruct` no Ollama).
  - Teste rÃ¡pido de latÃªncia/saÃ­da (comparar baseline vs. LLM local).
  - Ajustar fallback de forma transparente (mensagem na UI).
  
## DiÃ¡rio de Trabalho â€” 27/08/2025

- **IntegraÃ§Ã£o Docker + Streamlit + Ollama**
  - Estruturamos o projeto com docker-compose contendo dois serviÃ§os:
  - app: Streamlit rodando a interface;
  - llm: container com Ollama rodando localmente.
  - O app.py agora seleciona o backend via variÃ¡vel LLM_BACKEND no .env.
  - O usuÃ¡rio pode trocar dinamicamente o backend nas configuraÃ§Ãµes da interface (com reset de sessÃ£o).
- **Modelos menores testados**
  - Exploramos opÃ§Ãµes menores que o mistral:7b, como llama3.2:3b-instruct e phi3:mini-4k-instruct, para reduzir tempo de inferÃªncia em CPU.
  - Ajustamos o Compose e .env para alternar facilmente entre modelos.
- **Tratamento de erros de desenvolvimento**
  - Timeout aumentado e streaming ativado para nÃ£o travar em respostas longas.
- **prompt engineering**
  - Reescrevemos o PROMPT usando abordagem COSTAR
  - SituaÃ§Ã£o: texto da aula Ã© sÃ³ base;
  - Tarefa: gerar 1 pergunta objetiva;
  - Resultado: saÃ­da curta, clara e sem poluiÃ§Ã£o.
- **Leitura Papers** (selecionados na pesquisa no dia anterior):
  1) Li, K., & Zhang, Y. (2024). *Planning First, Question Second: An LLM-Guided Method for Controllable Question Generation* (ACL Findings 2024).
  2) Mucciaccia, S. S., et al. (2025). *Automatic Multiple-Choice Question Generation and Evaluation Systems Based on LLM* (COLING 2025).
  3) Tan, B., et al. (2025). *A Review of Automatic Item Generation Techniques Leveraging Large Language Models* (IJATE 2025).

---

## Como rodar (rascunho)
1. Se desejar, edite o .env para configurar o backend e modelo desejado:
```
# usar backend mock (sem LLM real)
LLM_BACKEND=mock

# ou usar Ollama (modelo local via container llm)
LLM_BACKEND=ollama
OLLAMA_HOST=http://llm:11434
OLLAMA_MODEL=llama3.2:3b
```

2. Suba os serviÃ§os (app + llm):
```
docker compose up -d --build
```

3. Acesse a aplicaÃ§Ã£o:
```
http://localhost:8501
```
